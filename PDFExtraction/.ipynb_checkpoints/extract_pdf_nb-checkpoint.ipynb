{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2512a24b-d9f1-4416-b848-927de99879ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9f53e191-2a17-404d-9c22-5337c9320457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_of_contents_page_number(doc):\n",
    "    \n",
    "    print(f\"total pages :{doc.page_count}\")\n",
    "    toc_page_num_array=[]#represents the table of contents(TOC) page number\n",
    "    \n",
    "    #get the table of contents page number  \n",
    "    for i in range (0,doc.page_count):\n",
    "        current_page= doc.load_page(i)\n",
    "        \n",
    "        extracted_text=current_page.get_text()\n",
    "        #print(f\"current page number:{i}\")\n",
    "        #print(extracted_text)\n",
    "    \n",
    "        regex_table_of_content=r'(table\\s+of\\s+contents)|(contents)|(in\\s+this\\s+)/gi'\n",
    "\n",
    "        #There might be multiple table of contents as well in the pdf, so consider all of them to create one single dictionary\n",
    "        if re.findall(regex_table_of_content,extracted_text.lower()):\n",
    "            print(f\"table of contents on page {i}\")\n",
    "            toc_page_num_array.append(i)\n",
    "    return toc_page_num_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b18a6e3-8131-4b8f-ab52-96cbc5c22b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toc_dict_for_pdf(toc_page_num_array,doc):\n",
    "\n",
    "    #a list of created toc dictionaries after all the preprocessing for multiple TOCs in a single pdf\n",
    "    toc_dictionaries_array=[]\n",
    "\n",
    "    for each_page_number in toc_page_num_array:\n",
    "        \n",
    "        toc_page_text= doc.load_page(each_page_number).get_text()\n",
    "        #print(toc_page_text)\n",
    "        \n",
    "        # a dict to hold the headings and their page number\n",
    "        dict_headings_for_current_pdf={}\n",
    "    \n",
    "        regex_for_presence_of_digits=r'(\\d+)'\n",
    "    \n",
    "        preprocessed_text_1=preprocess_step_1(toc_page_text)\n",
    "        preprocessed_text_array_2= preprocess_step_2(preprocessed_text_1,regex_for_presence_of_digits) \n",
    "        preprocessed_text_array_3= preprocess_step_3(preprocessed_text_array_2,regex_for_presence_of_digits)\n",
    "        toc_order=encode_the_pattern_and_determine_the_toc_order(preprocessed_text_array_3)\n",
    "    \n",
    "        if(toc_order!=None):\n",
    "            (toc_pattern,actual_order_array)=toc_order\n",
    "            dict_headings_for_current_pdf=create_the_toc_dictionary(actual_order_array,preprocessed_text_array_3,toc_pattern)\n",
    "    \n",
    "            #add the dictonary to all dictonaries list\n",
    "            toc_dictionaries_array.append(dict_headings_for_current_pdf)\n",
    "\n",
    "    \n",
    "    if(len(toc_dictionaries_array)>0):\n",
    "        return toc_dictionaries_array\n",
    "    else:\n",
    "        return None\n",
    "        '''\n",
    "        regex_for_heading_and_its_page_num=r'^([^\\.]+)([^a-zA-Z\\d]+)([\\d\\s]+)$'\n",
    "        checking_regex= re.match(regex_for_heading_and_its_page_num, toc_page_text)\n",
    "        print(type(checking_regex))\n",
    "        \n",
    "        if checking_regex:\n",
    "            print(f\"heading and its page number is present\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"no headings present\")\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bd028f2-3c3a-496e-bf9e-4bf750cf2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_step_1(toc_page_text):\n",
    "    '''In this step the following preprocessing is performed:-\n",
    "    1. The dots from the headings\n",
    "    2. The phrases \"table of contents, index, page, contents, etc\" are removed\n",
    "\n",
    "    Then the cleaned text is returned\n",
    "    '''\n",
    "    \n",
    "     #split the table of contents page by new line character\n",
    "    lines_array= toc_page_text.split('\\n')\n",
    "    \n",
    "    ###################################################################################################\n",
    "    #clean the contents. like removing the dots from the table of contents,etc.\n",
    "    #remove the dots from the table of contents sentences eg:heading.................46\n",
    "    cleaned_table_of_contents=[]\n",
    "    \n",
    "    for sentence in lines_array:\n",
    "        #split each sentence to give words in each sentence\n",
    "        if sentence.strip().lower() not in [\"table of contents\",\"page\",\"index\",\"contents\"]:\n",
    "            inner_split_arr= sentence.split()\n",
    "            cleaned_sentence_arr=[]\n",
    "            for word in inner_split_arr:\n",
    "                if word.strip() !=\".\" and word !=\" \":\n",
    "                    cleaned_sentence_arr.append(word.strip())\n",
    "        \n",
    "            print(cleaned_sentence_arr)\n",
    "            joined_cleaned_sentence= ' '.join(cleaned_sentence_arr)\n",
    "            if len(cleaned_sentence_arr)>0:\n",
    "                cleaned_table_of_contents.append(joined_cleaned_sentence)\n",
    "    \n",
    "    print()\n",
    "    #print(cleaned_table_of_contents)  \n",
    "    return cleaned_table_of_contents\n",
    "    ###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b5abca77-a85b-434c-a37b-36fdd97766e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_step_2(preprocessed_text_1,regex_for_presence_of_digits):\n",
    "    '''\n",
    "    The following preprocessing is performed\n",
    "    We are trying to find out the sentences which are the table of contents and separate them from any other text present.This is required due to the issue caused by the Abbott pdf.Refer \"pdfs\" directory to see the pdf\n",
    "\n",
    "    Steps are:-\n",
    "    1.check whether there is a digit in the sentence\n",
    "    2. if yes then consider the sentence\n",
    "    3. if no then check whether the last sentence contains a digit \n",
    "    4. if yes then consider it\n",
    "    5. if no , then check whether the next sentence contains the digit, if yes then consider it\n",
    "    '''\n",
    "\n",
    "    #list store the preprocessed text \n",
    "    preprocessed_text_array=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    #iterate over the preprocessed text from step 1\n",
    "    for index,sentence in enumerate(preprocessed_text_1):\n",
    "        #if there is a digit in the sentence then consider it\n",
    "        if re.findall(regex_for_presence_of_digits,sentence):\n",
    "            preprocessed_text_array.append(sentence)\n",
    "\n",
    "        #if there is no digit but the previous sentence contains a digit then consider it\n",
    "        elif re.findall(regex_for_presence_of_digits,preprocessed_text_1[index-1]):\n",
    "            #print(index)\n",
    "            #print(f\" index is {index} ,current sentence:{sentence} sentence at previous index: {cleaned_table_of_contents[index-1]}\")\n",
    "            preprocessed_text_array.append(sentence)\n",
    "\n",
    "        #if the next sentence contains a digit then consider it\n",
    "        elif index < len(preprocessed_text_1)-1:\n",
    "            if re.findall(regex_for_presence_of_digits,preprocessed_text_1[index+1]):\n",
    "                preprocessed_text_array.append(sentence)\n",
    "       \n",
    "    print()\n",
    "    print()\n",
    "    #print(cleaned_split_array)\n",
    "    return preprocessed_text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "111474f5-fa50-46fc-91bf-55bc4d566fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_step_3(preprocessed_text_2,regex_for_presence_of_digits):\n",
    "    '''\n",
    "    Here, after getting the sentences that are part of table of contents, we preprocess them in the following ways\n",
    "    1. For sentences that contain a digit,split them using the digit. This is done to identify whether the digit in the sentence is the page number or is part of the heading\n",
    "    After splitting them using the digit, these sentences will be analyzed further to determine whether the digit is the page number or not.\n",
    "    2. Remove any empty strings that are present in them\n",
    "    '''\n",
    "\n",
    "    \n",
    "    preprocessed_text_array=[]\n",
    "    final_preprocessed_text_array=[]\n",
    "    \n",
    "    #1. iterate over 'cleaned_split_array' elements and split the sentences that contain digits to find the pattern\n",
    "    for index,sentence in enumerate(preprocessed_text_2):\n",
    "        if re.findall(regex_for_presence_of_digits,sentence):\n",
    "            split_sentence= re.split(r'^(\\s*\\d+)|(\\d+\\s*)$',sentence) #need to test this regex\n",
    "            #print(split_sentence)\n",
    "            preprocessed_text_array.extend(split_sentence)\n",
    "        else:\n",
    "            preprocessed_text_array.append(sentence)\n",
    "    print()\n",
    "    #print(new_cleaned_split_array)\n",
    "\n",
    "    \n",
    "    #2. remove any empty strings\n",
    "    for sentence in preprocessed_text_array:\n",
    "        if sentence!=\"\" and sentence!=\" \" and sentence!=None and sentence!=\".\":\n",
    "            final_preprocessed_text_array.append(sentence)\n",
    "    print()\n",
    "    return final_preprocessed_text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e052592d-d4f1-49de-bf65-775a69203b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_the_pattern_and_determine_the_toc_order(preprocessed_text_array_3):\n",
    "    '''\n",
    "    Here the aim is to find the order in which the heading and the page number are arranged. \n",
    "    Possible orders can be :1. Heading......24(page number) or 24....Heading\n",
    "    Hence to determine this, we encode the string and the digits\n",
    "    Encoding :-  0=> digit,1=> string\n",
    "    Then we remove the duplicates by replacing similar consecutive elements with a single value ie. [1,1,1,1,0 ]=>[1,0] (all 1s are replaced with a single one)\n",
    "    Then to determine the Order, we consider the last two elements in the list.This is considered to be the represent order accurately. This is based on my observation and may not be accurate for all pdfs, but it works for the currently considered pdf.\n",
    "    \n",
    "    '''\n",
    "    #.find the pattern. 0 => digit,1=> word\n",
    "    toc_pattern=[]\n",
    "    for sentence in preprocessed_text_array_3:\n",
    "        if sentence.strip().isdigit():\n",
    "            toc_pattern.append(0)\n",
    "        else:\n",
    "            toc_pattern.append(1)\n",
    "    print(toc_pattern)\n",
    "    \n",
    "    \n",
    "    cleaned_toc_pattern=[]\n",
    "    for index, element in enumerate(toc_pattern):\n",
    "        if index==0:\n",
    "            cleaned_toc_pattern.append(element)\n",
    "        elif toc_pattern[index-1]!=element:\n",
    "            cleaned_toc_pattern.append(element)\n",
    "    print(cleaned_toc_pattern)\n",
    "    \n",
    "    \n",
    "    #check the ending element in the cleaned toc pattern. This signifies whether the ending element is string or digit and assumption is that it is correct ie. if its a digit here it means that it is a page number in the pdf as well\n",
    "    #the last pair of elements is the actual order\n",
    "\n",
    "    if len(cleaned_toc_pattern)>=2:\n",
    "        actual_order_array=[cleaned_toc_pattern[-2], cleaned_toc_pattern[-1]]\n",
    "        print(actual_order_array)\n",
    "        return (toc_pattern,actual_order_array)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d0641fe-1de6-4a60-84fd-0e3354475a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_the_toc_dictionary(actual_order_array,preprocessed_text_array_3,toc_pattern):\n",
    "\n",
    "    '''\n",
    "    Here we create the dictionary that holds the page number and the heading\n",
    "    '''\n",
    "    dict_headings={}\n",
    "    #create the dictionary\n",
    "    for index,sentence in enumerate(preprocessed_text_array_3):\n",
    "        if toc_pattern[index]==actual_order_array[0] and toc_pattern[index+1]==actual_order_array[1]:\n",
    "            dict_headings[preprocessed_text_array_3[index]]=preprocessed_text_array_3[index+1]\n",
    "    #all_toc_dicts[pdf]=dict_headings\n",
    "    print(dict_headings)\n",
    "    return dict_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e5e3164-1c68-4ac7-aa31-f30014582fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#create the dictionary\\nfor i in range(0,len(cleaned_table_of_contents)):\\n    if(i!=len(cleaned_table_of_contents)-1):\\n        y=i+1\\n    else:\\n        break\\n    if cleaned_table_of_contents[y].isdigit():\\n        dict_headings[int(cleaned_table_of_contents[y])]=cleaned_table_of_contents[i]\\nprint()\\nprint(dict_headings)\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#TRASH CODE\n",
    "#determine where are the page numbers with respect to the headings\n",
    "\n",
    "#determine pattern. Run the loop in reverse\n",
    "'''\n",
    "index= len(cleaned_toc_pattern)-1\n",
    "selected_element=cleaned_toc_pattern[index]#select the last element . Here element means either digit or string ie 0 or 1\n",
    "current_pattern=[selected_element]\n",
    "current_pattern_len=2\n",
    "count_digit_after_string=0\n",
    "count_string_after_digit=0\n",
    "\n",
    "while(index>=0):\n",
    "    if toc_pattern[index]==0 and toc_pattern[index+1] == 1 :\n",
    "        count_string_after_digit+=1\n",
    "    elif toc_pattern[index]==1 and toc_pattern[index+1] == 0:\n",
    "        count_digit_after_string+=1\n",
    "    \n",
    "    index+=2\n",
    "\n",
    "print(f\"Digit after string count:{count_digit_after_string}\")\n",
    "print(f\"String after digit count:{count_string_after_digit}\")\n",
    "'''\n",
    "'''\n",
    "#create the dictionary\n",
    "for i in range(0,len(cleaned_table_of_contents)):\n",
    "    if(i!=len(cleaned_table_of_contents)-1):\n",
    "        y=i+1\n",
    "    else:\n",
    "        break\n",
    "    if cleaned_table_of_contents[y].isdigit():\n",
    "        dict_headings[int(cleaned_table_of_contents[y])]=cleaned_table_of_contents[i]\n",
    "print()\n",
    "print(dict_headings)\n",
    "'''\n",
    "\n",
    "#print(all_toc_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f362726-dad5-4a9d-ae60-4c6b8f06ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnnualReport1.pdf', 'abbott_2023_annual_report.pdf', 'wipo_pub_rn2021_18e.pdf', 'birac_annual_report_2012.pdf', 'nestle_annual_report_2023.pdf', 'colgate_annual_report_2023.pdf', 'Netflix_annual_report_2023.pdf', 'netflix_ar_2022.pdf', 'netflix_ar_2003.pdf', 'netflix_ar_2004.pdf', 'netflix_ar_2005.pdf', 'netflix_ar_2021.pdf', 'netflix_ar_2006.pdf', 'netflix_ar_2019.pdf', 'meta_ar_2021.pdf', 'meta_ar_2020.pdf', 'meta_ar_2019.pdf', 'meta_ar_2012.pdf', 'pfizer_ar_2023.pdf', 'tata_group_ar_2024.pdf', 'tata_motors_ar_2022.pdf', 'coca_cola_ar_2023.pdf', 'apple_ar_2023.pdf']\n",
      "total pages :235\n",
      "table of contents on page 1\n",
      "table of contents on page 24\n",
      "['DIRECTORS']\n",
      "['3']\n",
      "['BOARD’S', 'REPORT']\n",
      "['4']\n",
      "['ANNEXURES', 'TO', 'BOARD’S', 'REPORT']\n",
      "['11']\n",
      "['STANDALONE', 'FINANCIAL', 'STATEMENTS']\n",
      "['INDEPENDENT', 'AUDITORS’', 'REPORT']\n",
      "['37']\n",
      "['ANNEXURE', 'TO', 'INDEPENDENT', 'AUDITORS’', 'REPORT']\n",
      "['42']\n",
      "['BALANCE', 'SHEET']\n",
      "['51']\n",
      "['STATEMENT', 'OF', 'PROFIT', '&', 'LOSS']\n",
      "['52']\n",
      "['CASH', 'FLOW', 'STATEMENT']\n",
      "['54']\n",
      "['STATEMENT', 'OF', 'CHANGES', 'IN', 'EQUITY']\n",
      "['56']\n",
      "['NOTES', 'FORMING', 'PART', 'OF', 'THE', 'FINANCIAL', 'STATEMENTS']\n",
      "['58']\n",
      "['CONSOLIDATED', 'FINANCIAL', 'STATEMENTS']\n",
      "['INDEPENDENT', 'AUDITORS’', 'REPORT']\n",
      "['134']\n",
      "['ANNEXURE', 'TO', 'INDEPENDENT', 'AUDITORS’', 'REPORT']\n",
      "['141']\n",
      "['BALANCE', 'SHEET']\n",
      "['144']\n",
      "['STATEMENT', 'OF', 'PROFIT', '&', 'LOSS']\n",
      "['145']\n",
      "['CASH', 'FLOW', 'STATEMENT']\n",
      "['147']\n",
      "['STATEMENT', 'OF', 'CHANGES', 'IN', 'EQUITY']\n",
      "['149']\n",
      "['NOTES', 'FORMING', 'PART', 'OF', 'THE', 'FINANCIAL', 'STATEMENTS']\n",
      "['151']\n",
      "['INDUSTRIES']\n",
      "['Seventy-ninth', 'annual', 'report']\n",
      "['Tata', 'Industries', 'Limited']\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[0, 1]\n",
      "{'3': 'BOARD’S REPORT', '4': 'ANNEXURES TO BOARD’S REPORT', '11': 'STANDALONE FINANCIAL STATEMENTS', '37': 'ANNEXURE TO INDEPENDENT AUDITORS’ REPORT', '42': 'BALANCE SHEET', '51': 'STATEMENT OF PROFIT & LOSS', '52': 'CASH FLOW STATEMENT', '54': 'STATEMENT OF CHANGES IN EQUITY', '56': 'NOTES FORMING PART OF THE FINANCIAL STATEMENTS', '58': 'CONSOLIDATED FINANCIAL STATEMENTS', '134': 'ANNEXURE TO INDEPENDENT AUDITORS’ REPORT', '141': 'BALANCE SHEET', '144': 'STATEMENT OF PROFIT & LOSS', '145': 'CASH FLOW STATEMENT', '147': 'STATEMENT OF CHANGES IN EQUITY', '149': 'NOTES FORMING PART OF THE FINANCIAL STATEMENTS', '151': 'INDUSTRIES'}\n",
      "['¶Annexure', 'A·']\n",
      "['To,']\n",
      "['The', 'Members']\n",
      "['TATA', 'INDUSTRIES', 'LIMITED']\n",
      "['Our', 'report', 'of', 'even', 'date', 'is', 'to', 'be', 'read', 'along', 'with', 'this', 'letter.']\n",
      "['1.']\n",
      "['Maintenance', 'of', 'secretarial', 'record', 'is', 'the', 'responsibility', 'of', 'the', 'management', 'of']\n",
      "['the', 'Company.', 'Our', 'responsibility', 'is', 'to', 'express', 'an', 'opinion', 'on', 'these', 'secretarial']\n",
      "['records', 'based', 'on', 'our', 'audit.']\n",
      "['2.']\n",
      "['We', 'have', 'followed', 'the', 'audit', 'practices', 'and', 'process', 'as', 'were', 'appropriate', 'to']\n",
      "['obtain', 'reasonable', 'assurance', 'about', 'the', 'correctness', 'of', 'the', 'contents', 'of', 'the']\n",
      "['secretarial', 'records.', 'The', 'verification', 'was', 'done', 'on', 'test', 'basis', 'to', 'ensure', 'that']\n",
      "['correct', 'facts', 'are', 'reflected', 'in', 'secretarial', 'records.', 'We', 'believe', 'that', 'the', 'process']\n",
      "['and', 'practices,', 'we', 'followed', 'provide', 'a', 'reasonable', 'basis', 'for', 'our', 'opinion.']\n",
      "['3.']\n",
      "['We', 'have', 'not', 'verified', 'the', 'correctness', 'and', 'appropriateness', 'of', 'financial', 'records']\n",
      "['and', 'Books', 'of', 'Accounts', 'of', 'the', 'Company.']\n",
      "['4.']\n",
      "['Where', 'ever', 'required,', 'we', 'have', 'obtained', 'the', 'Management', 'Representation']\n",
      "['about', 'the', 'Compliance', 'of', 'laws,', 'rules', 'and', 'regulations', 'and', 'happening', 'of', 'events']\n",
      "['etc.']\n",
      "['5.']\n",
      "['The', 'Compliance', 'of', 'the', 'provisions', 'of', 'Corporate', 'and', 'other', 'applicable', 'laws,']\n",
      "['rules,', 'regulations,', 'standards', 'is', 'the', 'responsibility', 'of', 'management.', 'Our']\n",
      "['examination', 'was', 'limited', 'to', 'the', 'verification', 'of', 'procedure', 'on', 'test', 'basis.']\n",
      "['6.']\n",
      "['The', 'Secretarial', 'Audit', 'report', 'is', 'neither', 'an', 'assurance', 'as', 'to', 'the', 'future', 'viability']\n",
      "['of', 'the', 'Company', 'nor', 'of', 'the', 'efficacy', 'or', 'effectiveness', 'with', 'which', 'the']\n",
      "['management', 'has', 'conducted', 'the', 'affairs', 'of', 'the', 'Company.']\n",
      "['For', 'Parikh', '&', 'Associates']\n",
      "['Company', 'Secretaries']\n",
      "['Place:', 'Mumbai']\n",
      "['Date:', '27.04.2023']\n",
      "['Signature:']\n",
      "['Jigyasa', 'N.', 'Ved']\n",
      "['Partner']\n",
      "['FCS', 'No:', '6488', 'CP', 'No:', '6018']\n",
      "['UDIN:', 'F006488E000203949']\n",
      "['PR', 'No.:', '1129/2021']\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0]\n",
      "{'Our report of even date is to be read along with this letter.': '1', 'records based on our audit.': '2', 'and practices, we followed provide a reasonable basis for our opinion.': '3', 'and Books of Accounts of the Company.': '4', 'etc.': '5', 'examination was limited to the verification of procedure on test basis.': '6', 'Date: 27.04.': '2023', 'FCS No: 6488 CP No: ': '6018', 'UDIN: F006488E': '000203949', 'PR No.: 1129/': '2021'}\n",
      "DICTIONARY FOR CURRENT PDF IS\n",
      "\n",
      "[{'3': 'BOARD’S REPORT', '4': 'ANNEXURES TO BOARD’S REPORT', '11': 'STANDALONE FINANCIAL STATEMENTS', '37': 'ANNEXURE TO INDEPENDENT AUDITORS’ REPORT', '42': 'BALANCE SHEET', '51': 'STATEMENT OF PROFIT & LOSS', '52': 'CASH FLOW STATEMENT', '54': 'STATEMENT OF CHANGES IN EQUITY', '56': 'NOTES FORMING PART OF THE FINANCIAL STATEMENTS', '58': 'CONSOLIDATED FINANCIAL STATEMENTS', '134': 'ANNEXURE TO INDEPENDENT AUDITORS’ REPORT', '141': 'BALANCE SHEET', '144': 'STATEMENT OF PROFIT & LOSS', '145': 'CASH FLOW STATEMENT', '147': 'STATEMENT OF CHANGES IN EQUITY', '149': 'NOTES FORMING PART OF THE FINANCIAL STATEMENTS', '151': 'INDUSTRIES'}, {'Our report of even date is to be read along with this letter.': '1', 'records based on our audit.': '2', 'and practices, we followed provide a reasonable basis for our opinion.': '3', 'and Books of Accounts of the Company.': '4', 'etc.': '5', 'examination was limited to the verification of procedure on test basis.': '6', 'Date: 27.04.': '2023', 'FCS No: 6488 CP No: ': '6018', 'UDIN: F006488E': '000203949', 'PR No.: 1129/': '2021'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nALL PDFS THAT ARE TESTED AND THEIR ORDER\\n[0:'AnnualReport1.pdf(working)', 1:'abbott_2023_annual_report.pdf(working)', '2:wipo_pub_rn2021_18e.pdf', 3:'birac_annual_report_2012.pdf', \\n4:'nestle_annual_report_2023.pdf', 5:'colgate_annual_report_2023.pdf(working)', 6:'Netflix_annual_report_2023.pdf', 7:'netflix_ar_2022.pdf', \\n8:'netflix_ar_2003.pdf', 9:'netflix_ar_2004.pdf', 10:'netflix_ar_2005.pdf', 11:'netflix_ar_2021.pdf', 12:'netflix_ar_2006.pdf', \\n13:'netflix_ar_2019.pdf', 14:'meta_ar_2021.pdf', 15:'meta_ar_2020.pdf', 16:'meta_ar_2019.pdf', 17:'meta_ar_2012.pdf', 18:'pfizer_ar_2023.pdf(contains roman numerals for page numbers hence there's an issue', \\n19:'tata_group_ar_2024.pdf', 20:'tata_motors_ar_2022.pdf', 21:'coca_cola_ar_2023.pdf(working)', 22:'apple_ar_2023.pdf(working)']\\n\\n\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory=\"../PDFExtraction/pdfs\"\n",
    "all_pdfs=os.listdir(directory)\n",
    "print(all_pdfs)\n",
    "all_toc_dicts={}\n",
    "\n",
    "\n",
    "#Single PDFs testing\n",
    "doc=fitz.open(os.path.join(directory,all_pdfs[19]))\n",
    "#doc= fitz.open(\"../PDFExtraction/pdfs/AnnualReport1.pdf\")\n",
    "#doc= fitz.open(\"../PDFExtraction/pdfs/abbott_2023_annual_report.pdf\")\n",
    "#for testing, it is a dict of all pdfs and their analyzed table of contents which is in a dict\n",
    "#doc=fitz.open(\"../PDFExtraction/pdfs/coca_cola_ar_2023.pdf\")\n",
    "toc_page_num=get_table_of_contents_page_number(doc)\n",
    "dict_for_current_pdf=get_toc_dict_for_pdf(toc_page_num,doc)\n",
    "\n",
    "if(dict_for_current_pdf !=None):\n",
    "    print(\"DICTIONARY FOR CURRENT PDF IS\\n\")\n",
    "    print(dict_for_current_pdf)\n",
    "\n",
    "\n",
    "'''\n",
    "#All PDFs testing\n",
    "for pdf in all_pdfs:\n",
    "    pdf_file_path=os.path.join(directory, pdf)\n",
    "    print(pdf)\n",
    "    doc=fitz.open(pdf_file_path)\n",
    "    toc_page_num=get_table_of_contents_page_number(doc)\n",
    "    dict_for_current_pdf=get_toc_dict_for_pdf(toc_page_num,doc)\n",
    "\n",
    "    if(dict_for_current_pdf !=None):\n",
    "        #add the dictionary to all dictonary list\n",
    "        all_toc_dicts[pdf]=dict_for_current_pdf\n",
    "    \n",
    "\n",
    "print(all_toc_dicts)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "ALL PDFS THAT ARE TESTED AND THEIR ORDER\n",
    "[0:'AnnualReport1.pdf(working)', 1:'abbott_2023_annual_report.pdf(working)', '2:wipo_pub_rn2021_18e.pdf', 3:'birac_annual_report_2012.pdf', \n",
    "4:'nestle_annual_report_2023.pdf', 5:'colgate_annual_report_2023.pdf(working)', 6:'Netflix_annual_report_2023.pdf', 7:'netflix_ar_2022.pdf', \n",
    "8:'netflix_ar_2003.pdf', 9:'netflix_ar_2004.pdf', 10:'netflix_ar_2005.pdf', 11:'netflix_ar_2021.pdf', 12:'netflix_ar_2006.pdf', \n",
    "13:'netflix_ar_2019.pdf', 14:'meta_ar_2021.pdf', 15:'meta_ar_2020.pdf', 16:'meta_ar_2019.pdf', 17:'meta_ar_2012.pdf', 18:'pfizer_ar_2023.pdf(contains roman numerals for page numbers hence there's an issue', \n",
    "19:'tata_group_ar_2024.pdf(wrong pattern detection)', 20:'tata_motors_ar_2022.pdf', 21:'coca_cola_ar_2023.pdf(working)', 22:'apple_ar_2023.pdf(working)']\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a452e-0cf9-4da9-82ca-b17a33428893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
