[
    {
        "label": "mongo",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mongo",
        "description": "mongo",
        "detail": "mongo",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "isExtraImport": true,
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "PorterStemmer",
        "importPath": "nltk.stem.porter",
        "description": "nltk.stem.porter",
        "isExtraImport": true,
        "detail": "nltk.stem.porter",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "get_response",
        "importPath": "main.chat",
        "description": "main.chat",
        "isExtraImport": true,
        "detail": "main.chat",
        "documentation": {}
    },
    {
        "label": "extract_pdf_from_url",
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "isExtraImport": true,
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "get_table_of_contents_page_number",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def get_table_of_contents_page_number(doc):\n    print(f\"total pages :{doc.page_count}\")\n    toc_page_num_array=[]#represents the table of contents(TOC) page number\n    for i in range (0,doc.page_count):\n        current_page= doc.load_page(i)\n        extracted_text=current_page.get_text()\n        #print(f\"current page number:{i}\")\n        #print(extracted_text)\n        regex_table_of_content=r'(table\\s+of\\s+contents)|(contents)|(in\\s+this\\s+)/gi'\n        #There might be multiple table of contents as well in the pdf, so consider all of them to create one single dictionary",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "get_toc_dict_for_pdf",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def get_toc_dict_for_pdf(toc_page_num_array,doc):\n    #a list of created toc dictionaries after all the preprocessing for multiple TOCs in a single pdf\n    toc_dictionaries_array=[]\n    for each_page_number in toc_page_num_array:\n        toc_page_text= doc.load_page(each_page_number).get_text()\n        #print(toc_page_text)\n        # a dict to hold the headings and their page number\n        dict_headings_for_current_pdf={}\n        regex_for_presence_of_digits=r'(\\d+)'\n        preprocessed_text_1=preprocess_step_1(toc_page_text)",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "preprocess_step_1",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def preprocess_step_1(toc_page_text):\n    '''In this step the following preprocessing is performed:-\n    1. The dots from the headings\n    2. The phrases \"table of contents, index, page, contents, etc\" are removed\n    Then the cleaned text is returned\n    '''\n     #split the table of contents page by new line character\n    lines_array= toc_page_text.split('\\n')\n    ###################################################################################################\n    #clean the contents. like removing the dots from the table of contents,etc.",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "preprocess_step_2",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def preprocess_step_2(preprocessed_text_1,regex_for_presence_of_digits):\n    '''\n    The following preprocessing is performed\n    We are trying to find out the sentences which are the table of contents and separate them from any other text present.This is required due to the issue caused by the Abbott pdf.Refer \"pdfs\" directory to see the pdf\n    Steps are:-\n    1.check whether there is a digit in the sentence\n    2. if yes then consider the sentence\n    3. if no then check whether the last sentence contains a digit \n    4. if yes then consider it\n    5. if no , then check whether the next sentence contains the digit, if yes then consider it",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "preprocess_step_3",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def preprocess_step_3(preprocessed_text_2,regex_for_presence_of_digits):\n    '''\n    Here, after getting the sentences that are part of table of contents, we preprocess them in the following ways\n    1. For sentences that contain a digit,split them using the digit. This is done to identify whether the digit in the sentence is the page number or is part of the heading\n    After splitting them using the digit, these sentences will be analyzed further to determine whether the digit is the page number or not.\n    2. Remove any empty strings that are present in them\n    '''\n    preprocessed_text_array=[]\n    final_preprocessed_text_array=[]\n    temp_preprocessed_text_array=[]",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_only_valid_words_from_headings",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_only_valid_words_from_headings(preprocessed_text_array_3):\n    final_preprocessed_text_array=[]\n    for sentence in preprocessed_text_array_3:\n        #regex to remove the words such as \"PART\" and \"Item 1A. etc\"\n        regex_to_remove_unnecessary_words=r'^([\\w\\s]+[\\da-zA-Z]+\\.)'\n        part_of_sentence_to_remove=re.findall(regex_to_remove_unnecessary_words,sentence)\n        split_sentence=re.split(regex_to_remove_unnecessary_words,sentence)\n        for part in split_sentence:\n            if part!=\"\" and not re.findall(regex_to_remove_unnecessary_words,part):\n                final_preprocessed_text_array.append(part.strip())",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "check_whether_consecutive_sentences_are_present",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def check_whether_consecutive_sentences_are_present(preproccessed_text_array_3):\n    value_to_return=False\n    for index,sentences in enumerate(preproccessed_text_array_3):\n        if index< len(preproccessed_text_array_3)-1:\n            if not preproccessed_text_array_3[index].isdigit() and not preproccessed_text_array_3[index+1].isdigit():\n                value_to_return=True\n                break\n            else:\n                value_to_return= False\n    return value_to_return",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "merge_consecutive_sentences_into_one",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def merge_consecutive_sentences_into_one(preprocessed_text_array_3):\n    temp_preprocessed_text_array=[]\n    index_to_skip=[]\n    for index,sentence in enumerate(preprocessed_text_array_3):\n        #check until 2nd last index else index out of range error will be thrown because we are using (index+1)\n        if index <len(preprocessed_text_array_3)-1:\n            #if this index has already been merged\n            if index in index_to_skip:\n                continue\n            #consecutive strings are present",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "encode_the_pattern",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def encode_the_pattern(preprocessed_text_array_3):\n    #.find the pattern. 0 => digit,1=> word\n    pattern=[]\n    for index, sentence in enumerate(preprocessed_text_array_3):\n        if sentence.isdigit():\n            pattern.append(0)\n        else:\n            pattern.append(1)\n    return pattern\n#10.",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "encode_the_pattern_and_determine_the_toc_order",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def encode_the_pattern_and_determine_the_toc_order(preprocessed_text_array_3):\n    '''\n    Here the aim is to find the order in which the heading and the page number are arranged. \n    Possible orders can be :1. Heading......24(page number) or 24....Heading\n    Hence to determine this, we encode the string and the digits\n    Encoding :-  0=> digit,1=> string\n    Then we remove the duplicates by replacing similar consecutive elements with a single value ie. [1,1,1,1,0 ]=>[1,0] (all 1s are replaced with a single one)\n    Then to determine the Order, we consider the last two elements in the list.This is considered to be the represent order accurately. This is based on my observation and may not be accurate for all pdfs, but it works for the currently considered pdf.\n    '''\n    #.find the pattern. 0 => digit,1=> word",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "create_the_toc_dictionary",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def create_the_toc_dictionary(actual_order_array,preprocessed_text_array_3,toc_pattern):\n    '''\n    Here we create the dictionary that holds the page number and the heading\n    '''\n    dict_headings={}\n    #create the dictionary\n    for index,sentence in enumerate(preprocessed_text_array_3):\n        if(index <len(preprocessed_text_array_3)-1):\n            #print(f\"toc_pattern at current index is:{toc_pattern[index]} and at next index is :{toc_pattern[index+1]}\")\n            if toc_pattern[index]==actual_order_array[0] and toc_pattern[index+1]==actual_order_array[1]:",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "calculate_offset",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def calculate_offset(toc_dict_array, doc,toc_page_num_array):\n   #go through each page and then check for the word on that page\n   for toc_dict in toc_dict_array:\n    all_headings_status_dict={}\n    #iterate over the dictionary\n    for heading,page_num in toc_dict.items():\n        #print(page_num, heading)\n        all_headings_status_dict[heading]={\"toc_page_num\":page_num,\"pdf_page_num\":[],\"offset\":0}\n        regex_for_current_heading=r''+heading.strip()+'\\s*(?![\\w\\d])'\n       # print(f\"regex is :{regex_for_current_heading}\")",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "fix_offset_issues",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def fix_offset_issues(all_headings_status_dict):\n    #find the offset that appears most often\n    offset_count_dict={}\n    for headings,page_numbers in all_headings_status_dict.items():\n        offset=page_numbers[\"offset\"]\n        if offset in offset_count_dict.keys():\n            offset_count_dict[offset]+=1\n        else:\n            offset_count_dict[offset]=1\n    offset_with_max_count=list(offset_count_dict.keys())[0]",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_subheadings_and_their_text",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_subheadings_and_their_text(doc,all_headings_status_dict):\n    modified_all_headings_status_dict= determine_starting_and_ending_page_numbers_for_each_heading(doc, all_headings_status_dict)  \n    #print(\"\\n the headings are:\\n\")\n    #pprint(modified_all_headings_status_dict)\n    #list of all the headings \n    all_headings_list=list(modified_all_headings_status_dict.keys())\n    extracted_text_dict={}\n    #now extract subheading\n    for index,heading in enumerate(all_headings_list):\n        extracted_text_dict[heading]=[]",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "determine_starting_and_ending_page_numbers_for_each_heading",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def determine_starting_and_ending_page_numbers_for_each_heading(doc, all_headings_status_dict):\n    modified_all_headings_status_dict= all_headings_status_dict\n    #pprint(all_headings_status_dict)\n    #go to the page with the headings and extract the text until the next heading\n    for index,(headings,page_numbers) in enumerate(all_headings_status_dict.items()):\n        #when we reach to the last ending, there is no heading after it hence dont increment the index\n        if index < len(list(all_headings_status_dict.keys()))-1:\n            next_heading_index=index+1\n            next_heading=list(all_headings_status_dict.keys())[next_heading_index]\n            current_heading=list(all_headings_status_dict.keys())[index]",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "does_current_heading_end_and_new_heading_start_on_same_page",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def does_current_heading_end_and_new_heading_start_on_same_page(doc,next_toc_heading, next_toc_heading_pdf_page_num):\n    #print(f\"the next toc heading pdf page number is:{next_toc_heading_pdf_page_num}\")\n    regex_for_next_toc_heading=r''+next_toc_heading+'(?![\\s\\w\\d])'\n    next_heading_page= doc.load_page(next_toc_heading_pdf_page_num)\n    blocks= next_heading_page.get_text(\"dict\",flags=11)[\"blocks\"]\n    #text_belonging_to_previous_toc_heading=[]\n    #print(f\"{next_toc_heading}\")\n    #print(f\"the first block dimensions\\n\")\n    #pprint(blocks[0])\n    value_to_return=False",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "flags_decomposer",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def flags_decomposer(flags):\n    \"\"\"Make font flags human readable.\"\"\"\n    l = []\n    if flags & 2 ** 0:\n        l.append(\"superscript\")\n    if flags & 2 ** 1:\n        l.append(\"italic\")\n    if flags & 2 ** 2:\n        l.append(\"serifed\")\n    else:",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_subheadings",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_subheadings(doc,starting_page_num,ending_page_num,current_toc_heading, next_toc_heading):\n    regex_for_current_toc_heading= r''+current_toc_heading+'(?![\\s\\w\\d])'\n    regex_for_next_toc_heading= r''+next_toc_heading+'(?![\\s\\w\\d])'\n    #another_regex_for_toc_heading=r'(?<=Item\\s\\w\\.\\s)'+next_toc_heading+'(?![\\s\\w\\d])'\n    #print(f\"Extract subheadings| current toc heading is:{current_toc_heading} and next toc heading is {next_toc_heading}\")\n    start_finding_subheadings=False #TODO:change back to false\n    next_subheading_found_in_a_paragraph=False\n    current_toc_heading_found=False\n    #possible subheadings in the heading\n    possible_subheadings_array={}",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "find_possible_subheadings",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def find_possible_subheadings(current_span_text,font, flags):\n    '''\n    Steps in analyzing are:\n    1. check whether the text is the same as the TOC heading, if yes, then don't consider it\n    2. check the font sizes of the text.The largest ones are the subheadings usually\n    3. check for keyword \"bold\" in the font and the flags of each block to determine the subheadings\n    '''\n    filter_2_block_that_might_contain_subheading=[]\n    all_fonts= font.split(\",\")\n    all_flags=flags.split(\",\")",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "remove_toc_headings_from_subheadings",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def remove_toc_headings_from_subheadings(toc_heading,possible_subheadings_array):\n    filtered_subheadings_array=[]\n    regex_for_toc_heading= r'(?<=Item\\s\\w\\w\\.\\s)'+toc_heading+'(?![\\s\\w\\d])'\n    another_regex_for_toc_heading=r'(?<=Item\\s\\w\\.\\s)'+toc_heading+'(?![\\s\\w\\d])'\n    #regex_for_toc_heading= r''+toc_heading\n    for subheading in possible_subheadings_array:\n        if re.findall(regex_for_toc_heading, subheading) or re.findall(another_regex_for_toc_heading,subheading):\n            #print(f\"\\n this is a toc heading {subheading}\\n\")\n            #blocks_that_might_contain_subheading.append()\n            continue",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_text_within_subheading",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_text_within_subheading(doc,subheading_dict,toc_heading,toc_heading_page_num,next_toc_heading,next_toc_heading_page_number):\n    all_page_numbers_array=list(subheading_dict.keys())\n    #print(all_page_numbers_array)\n    #remove all the keys whose values are empty\n    temp_page_array=[]\n    for page_number in all_page_numbers_array:\n        #print(len(subheading_dict[9]))\n        #print(page_number)\n        if len(subheading_dict[page_number])>0:\n            temp_page_array.append(page_number)",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "write_output_to_a_file",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def write_output_to_a_file(output):\n    '''\n    f= open(\"./file_output/toc_heading_5.txt\",\"w\")\n    f.write(str(output))\n    f.close()\n    print(\"write finished\")\n    '''\n    with open(\"./file_output/output.json\", \"w\") as json_file:\n        json.dump(output, json_file, indent=4)\n        print(\"write finished\")",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "download_pdf",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def download_pdf(url):\n    save_folder = \"../PDFExtraction/downloaded_pdfs\"\n    response = requests.get(url)\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    file_name = url.split('/')[-1]\n    save_path = os.path.join(save_folder, file_name)\n    with open(save_path, 'wb') as f:\n        f.write(response.content)\n    print(f\"PDF downloaded and saved to {save_path}\")",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_data_from_pdf",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_data_from_pdf(pdf_path):\n    print(pdf_path)\n    doc=fitz.open(pdf_path)\n    toc_page_num_array=get_table_of_contents_page_number(doc)\n    print(\"page num\")\n    print(toc_page_num_array)\n    dict_for_current_pdf=get_toc_dict_for_pdf(toc_page_num_array,doc)\n    if(dict_for_current_pdf !=None):\n        print(\"DICTIONARY FOR CURRENT PDF IS\\n\")\n        pprint(dict_for_current_pdf)",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_pdf_from_url",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_pdf_from_url(url):\n    pdf_saved_path = download_pdf(url)\n    extract_data_from_pdf(pdf_saved_path)\n'''\n######################################################################################################################################################################\n                                                                                    ACTUAL IMPLEMENTATION\n######################################################################################################################################################################\n'''\n# directory=\"../PDFExtraction/pdfs\"\n# all_pdfs=os.listdir(directory)",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "extract_data_from_pdf",
        "kind": 2,
        "importPath": "PDFExtraction.extract_pdf",
        "description": "PDFExtraction.extract_pdf",
        "peekOfCode": "def extract_data_from_pdf(pdf_path):\n    print(pdf_path)\n    doc=fitz.open(pdf_path)\n    toc_page_num=get_table_of_contents_page_number(doc)\n    dict_for_current_pdf=get_toc_dict_for_pdf(toc_page_num,doc)\n    if(dict_for_current_pdf !=None):\n        print(\"DICTIONARY FOR CURRENT PDF IS\\n\")\n        print(dict_for_current_pdf)\nextract_data_from_pdf()\n'''",
        "detail": "PDFExtraction.extract_pdf",
        "documentation": {}
    },
    {
        "label": "get_response",
        "kind": 2,
        "importPath": "main.chat",
        "description": "main.chat",
        "peekOfCode": "def get_response(msg) :\n    sentence = tokenize(msg) \n    print(sentence)\n    return sentence\nif __name__ == \"__main__\":\n    print(\"Let's chat! (type 'quit' to exit)\")\n    resp = get_response(\"hey hello how are youu\")\n    print(resp)",
        "detail": "main.chat",
        "documentation": {}
    },
    {
        "label": "current_directory",
        "kind": 5,
        "importPath": "main.chat",
        "description": "main.chat",
        "peekOfCode": "current_directory = os.getcwd()\nparent_directory = os.path.dirname(current_directory)\nsys.path.append(parent_directory+'/')\nfrom utils.nltk_utlis import tokenize\ndef get_response(msg) :\n    sentence = tokenize(msg) \n    print(sentence)\n    return sentence\nif __name__ == \"__main__\":\n    print(\"Let's chat! (type 'quit' to exit)\")",
        "detail": "main.chat",
        "documentation": {}
    },
    {
        "label": "parent_directory",
        "kind": 5,
        "importPath": "main.chat",
        "description": "main.chat",
        "peekOfCode": "parent_directory = os.path.dirname(current_directory)\nsys.path.append(parent_directory+'/')\nfrom utils.nltk_utlis import tokenize\ndef get_response(msg) :\n    sentence = tokenize(msg) \n    print(sentence)\n    return sentence\nif __name__ == \"__main__\":\n    print(\"Let's chat! (type 'quit' to exit)\")\n    resp = get_response(\"hey hello how are youu\")",
        "detail": "main.chat",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "def tokenize(sentence):\n    return nltk.word_tokenize(sentence)\n# Stemming Function: Reduce each word to its word stem or root form\ndef stem(word):\n    # Convert word to lowercase and stem it using Porter Stemmer\n    return stemmer.stem(word.lower())\n# Bag of Words Function: Generates a binary vector representing presence/absence of words\ndef bag_of_words(tokenized_sentence, words):\n    # Stem each word in the tokenized sentence\n    stemmed = [stem(word) for word in tokenized_sentence]",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "stem",
        "kind": 2,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "def stem(word):\n    # Convert word to lowercase and stem it using Porter Stemmer\n    return stemmer.stem(word.lower())\n# Bag of Words Function: Generates a binary vector representing presence/absence of words\ndef bag_of_words(tokenized_sentence, words):\n    # Stem each word in the tokenized sentence\n    stemmed = [stem(word) for word in tokenized_sentence]\n    # Initialize bag of words vector with zeros\n    bag = np.zeros(len(words), dtype=np.float32)\n    # Iterate through each word in the vocabulary",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "bag_of_words",
        "kind": 2,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "def bag_of_words(tokenized_sentence, words):\n    # Stem each word in the tokenized sentence\n    stemmed = [stem(word) for word in tokenized_sentence]\n    # Initialize bag of words vector with zeros\n    bag = np.zeros(len(words), dtype=np.float32)\n    # Iterate through each word in the vocabulary\n    for idx, w in enumerate(words):\n        # Check if stemmed word is present in the tokenized sentence\n        if w in stemmed:\n            # If present, set the corresponding index in bag to 1",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "stemmer",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "stemmer = PorterStemmer()\n# Download necessary NLTK data (word tokenizer) for tokenization\nnltk.download('punkt')\n# Tokenization Function: Splits a sentence into a list of words\ndef tokenize(sentence):\n    return nltk.word_tokenize(sentence)\n# Stemming Function: Reduce each word to its word stem or root form\ndef stem(word):\n    # Convert word to lowercase and stem it using Porter Stemmer\n    return stemmer.stem(word.lower())",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "sentence",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "sentence = \"The quick brown fox jumps over the lazy dog\"\nwords = [\"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"apple\", \"banana\", \"cherry\"]\n# Tokenize the sentence\ntokenized_sentence = tokenize(sentence)\nprint(\"Tokenized sentence:\", tokenized_sentence)\n# Stem each word in the tokenized sentence\nstemmed_words = [stem(word) for word in tokenized_sentence]\nprint(\"Stemmed words:\", stemmed_words)\n# Generate bag-of-words representation\nbag_of_words_vector = bag_of_words(tokenized_sentence, words)",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "words",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "words = [\"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"apple\", \"banana\", \"cherry\"]\n# Tokenize the sentence\ntokenized_sentence = tokenize(sentence)\nprint(\"Tokenized sentence:\", tokenized_sentence)\n# Stem each word in the tokenized sentence\nstemmed_words = [stem(word) for word in tokenized_sentence]\nprint(\"Stemmed words:\", stemmed_words)\n# Generate bag-of-words representation\nbag_of_words_vector = bag_of_words(tokenized_sentence, words)\nprint(\"Bag of words vector:\", bag_of_words_vector)",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "tokenized_sentence",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "tokenized_sentence = tokenize(sentence)\nprint(\"Tokenized sentence:\", tokenized_sentence)\n# Stem each word in the tokenized sentence\nstemmed_words = [stem(word) for word in tokenized_sentence]\nprint(\"Stemmed words:\", stemmed_words)\n# Generate bag-of-words representation\nbag_of_words_vector = bag_of_words(tokenized_sentence, words)\nprint(\"Bag of words vector:\", bag_of_words_vector)",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "stemmed_words",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "stemmed_words = [stem(word) for word in tokenized_sentence]\nprint(\"Stemmed words:\", stemmed_words)\n# Generate bag-of-words representation\nbag_of_words_vector = bag_of_words(tokenized_sentence, words)\nprint(\"Bag of words vector:\", bag_of_words_vector)",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "bag_of_words_vector",
        "kind": 5,
        "importPath": "utils.nltk_utlis",
        "description": "utils.nltk_utlis",
        "peekOfCode": "bag_of_words_vector = bag_of_words(tokenized_sentence, words)\nprint(\"Bag of words vector:\", bag_of_words_vector)",
        "detail": "utils.nltk_utlis",
        "documentation": {}
    },
    {
        "label": "hello",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def hello():\n    return jsonify({'message': 'Hello, World!'})\n# API route for extraction\n@app.route('/api/extraction',methods=['POST'])\ndef extract_pdf():\n    print(\"yeee\")\n    url = request.get_json().get(\"url\")\n    id = request.get_json().get(\"id\")\n    extract_pdf_from_url(url)\n    message = {\"message\": \"Your url reached!!\",\"url\":url}",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "extract_pdf",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def extract_pdf():\n    print(\"yeee\")\n    url = request.get_json().get(\"url\")\n    id = request.get_json().get(\"id\")\n    extract_pdf_from_url(url)\n    message = {\"message\": \"Your url reached!!\",\"url\":url}\n    return jsonify(message)\n# API route for prediction\n@app.route('/api/predict',methods=['POST'])\ndef predict():",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def predict():\n    text = request.get_json().get(\"message\")\n    response = get_response(text)\n    message = {\"answer\": response}\n    return jsonify(message)\nif __name__ == '__main__':\n    app.run(debug=True)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = Flask(__name__)\n# Define a sample route for your API\n@app.route('/api/hello', methods=['GET'])\ndef hello():\n    return jsonify({'message': 'Hello, World!'})\n# API route for extraction\n@app.route('/api/extraction',methods=['POST'])\ndef extract_pdf():\n    print(\"yeee\")\n    url = request.get_json().get(\"url\")",
        "detail": "app",
        "documentation": {}
    }
]